{% docs __snowplow_web__ %}

{% raw %}

# Snowplow Web Package

Welcome to the documentation site for the Snowplow web dbt package. The package contains is a fully incremental model, that transforms raw web event data generated by the [Snowplow JavaScript tracker](https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/javascript-trackers/) into a series of derived tables of varying levels of aggregation.

**Note this doc site is linked to latest release of the package. If you are not using the latest release, [generate and serve](https://docs.getdbt.com/reference/commands/cmd-docs#dbt-docs-serve) the doc site locally for accurate documentation.**

## Overview

This model consists of a series of modules, each producing a table which serves as the input to the next module. The 'standard' modules are:

- Base: Performs the incremental logic, outputting the table `snowplow_web_base_events_this_run` which contains a de-duped data set of all events required for the current run of the model.
- Page Views: Aggregates event level data to a page view level, `page_view_id`.
- Sessions: Aggregates page view level data to a session level, `domain_sessionid`.
- Users: Aggregates session level data to a users level, `domain_userid`.
- User Mapping: Provides a mapping between user identifiers, `domain_userid` and `user_id`. This can be used for session stitching.

The 'standard' modules can be thought of as source code for the core logic of the model, which Snowplow maintains. These modules carry out the incremental logic in such a way as custom modules can be written to plug into the model's structure, without needing to write a parallel incremental logic. We recommend that all customisations are written in this way, which allows us to safely maintain and roll out updates to the model, without impact on dependent custom sql.

Each module produces a table which acts as the input to the subsequent module (the `_this_run` tables), and updates a derived table - with the exception of the Base module, which takes atomic data as its input, and does not update a derived table.

## Adapter Support

The Snowplow Web v0.5.1 package currently supports BigQuery, Redshift, Snowflake & Postgres.

## Installation

Check [dbt Hub](https://hub.getdbt.com/snowplow/snowplow_web/latest/) for the latest installation instructions, or read the [dbt docs][dbt-package-docs] for more information on installing packages.

## Quick Start

### 1 - Check source data

This package will by default will assume your Snowplow events data is contained in the `atomic` schema of your [target.database](https://docs.getdbt.com/docs/running-a-dbt-project/using-the-command-line-interface/configure-your-profile). In order to change this, please add the following to your `dbt_project.yml` file:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__atomic_schema: schema_with_snowplow_events
    snowplow__database: database_with_snowplow_events
```

### 2 - Enabled desired contexts

The web package has the option to join in data from the following 3 Snowplow enrichments:

- [IAB enrichment](https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/iab-enrichment/)
- [UA Parser enrichment](https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/ua-parser-enrichment/)
- [YAUAA enrichment](https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/yauaa-enrichment/)

By default these are **all disabled** in the web package. Assuming you have the enrichments turned on in your Snowplow pipeline, to enable the contexts within the package please add the following to your `dbt_project.yml` file:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__enable_iab: true
    snowplow__enable_ua: true
    snowplow__enable_yauaa: true
```

### 3 - Filter your data set

You can specify both `start_date` at which to start processing events and the `app_id`'s to filter for. By default the `start_date` is set to `2020-01-01` and all `app_id`'s are selected. To change this please add the following to your `dbt_project.yml` file:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__start_date: 'yyyy-mm-dd'
    snowplow__app_id: ['my_app_1','my_app_2']
```

#### BigQuery Only

Verify which column your events table is partitioned on. It will likely be partitioned on `collector_tstamp` or `derived_tstamp`. If it is partitioned on `collector_tstamp` you should set `snowplow__derived_tstamp_partitioned` to `false`. This will ensure only the `collector_tstamp` column is used for partition pruning when querying the events table:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__derived_tstamp_partitioned: false
```

### 4 - Verify page ping variables

The web package processes page ping events to calculate web page engagement times. If your [tracker configuration](https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/javascript-trackers/javascript-tracker/javascript-tracker-v3/tracking-events/#activity-tracking-page-pings) for `min_visit_length` (default 5) and `heartbeat` (default 10) differs from the defaults provided in this package, you can override by adding to your `dbt_project.yml`:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__min_visit_length: 5 # Default value
    snowplow__heartbeat: 10 # Default value
```

## Configuration

### Output Schemas

By default all scratch/staging tables will be created in the `<target.schema>_scratch` schema, the derived tables (`snowplow_web_page_views`, `snowplow_web_sessions`, `snowplow_web_users`) will be created in `<target.schema>_derived` and all manifest tables in `<target.schema>_snowplow_manifest`. To change, please add the following to your `dbt_project.yml` file:

```yml
# dbt_project.yml
...
models:
  snowplow_web:
    base:
      manifest:
        +schema: my_manifest_schema
      scratch:
        +schema: my_scratch_schema
    page_views:
      +schema: my_derived_schema
      scratch:
        +schema: my_scratch_schema
    sessions:
      +schema: my_derived_schema
      scratch:
        +schema: my_scratch_schema
    users:
      +schema: my_derived_schema
      scratch:
        +schema: my_scratch_schema
```

### Disabling a standard module

If you do not require certain modules provided by the package you have the option to disable them. For instance to disable the users module:

```yml
# dbt_project.yml
...
models:
  snowplow_web:
    users:
      enabled: false
```

Note that any dependent modules will also need to be disabled - for instance if you disabled the sessions module, you will also have to disable the users module.

### Further Configuration

This package makes use of a series of other variables, which are all set to the recommend values for the operation of the web model. Depending on your use case, you might want to override these values by adding to your `dbt_project.yml` file.

`snowplow__lookback_window_hours`:  Default 6. The number of hours to look before the latest event processed - to account for late arriving data, which comes out of order.

`snowplow__backfill_limit_days`:    Default 30. The maximum numbers of days of new data to be processed since the latest event processed. Please refer to the back-filling section for more details.

`snowplow__session_lookback_days`:  Default 365. Number of days to limit scan on snowplow_web_base_sessions_lifecycle_manifest manifest. Exists to improve performance of model when we have a lot of sessions. Should be set to as large a number as practical.

`snowplow__days_late_allowed`:      Default 3. The maximum allowed number of days between the event creation and it being sent to the collector. Exists to reduce lengthy table scans that can occur as a result of late arriving data.

`snowplow__max_session_days`:       Default 3. The maximum allowed session length in days. For a session exceeding this length, all events after this limit will stop being processed. Exists to reduce lengthy table scans that can occur due to long sessions which are usually a result of bots.

`snowplow__upsert_lookback_days`:   Default 30. Number of day to look back over the incremental derived tables during the upsert. Where performance is not a concern, should be set to as long a value as possible. Having too short a period can result in duplicates. Please see the incremental materialization section for more details.

`snowplow__ua_bot_filter`:          Default `True`. Configuration to filter out bots via the useragent string pattern match.

`snowplow__sessions_table`:         Default `{{ ref('snowplow_web_sessions') }}`. The users module requires data from the derived sessions table. If you choose to disable the standard sessions table in favor of your own custom table, set this to reference your new table e.g. `{{ ref('snowplow_web_sessions_custom') }}`. Please see the [README](https://github.com/snowplow/dbt-snowplow-web/tree/main/custom_example) in the `custom_example` directory for more information on this sort of implementation.

`snowplow__has_log_enabled`:        Default `True`. When executed, the package logs information about the current run to the CLI. This can be disabled by setting to `false`.

`snowplow__incremental_materialization`: Default `snowplow_incremental`. The materialization used for all incremental models within the package. `snowplow_incremental` builds upon the default incremental materialization provided by dbt, improving performance when modeling event data. If however you prefer to use the native dbt incremental materialization, or any other, then adjust accordingly.

`snowplow__allow_refresh`:          Default `False`. Used as the default value to return from the `allow_refresh()` macro. This macro determines whether the manifest tables can be refreshed or not, depending on your environment. See the 'Manifest Tables' section for more details.

`snowplow__dev_target_name`:        Default: `dev`. The [target name](https://docs.getdbt.com/reference/profiles.yml) of your development environment as defined in your `profiles.yml` file. See the 'Manifest Tables' section for more details.

`snowplow__session_stitching`:      Default: `True`. Determines whether to apply the user mapping to the sessions table. Please see the 'User Mapping' section for more details.

## YAML Selectors

Within this package we have provided a suite of suggested selectors to run and test the models within the package. This leverages dbt's [selector flag][dbt-selectors].

The selectors include:

- `snowplow_web`: Recommended way to run the package. This selection includes all models within the Snowplow Web as well as any custom models you have created.
- `snowplow_web_lean_tests`: Recommended way to test the models within the package. See the testing section for more details.

These are defined in the [`selectors.yml` file][selectors-yml-file] within the package, however in order to use these selections you will need to copy this file into your own dbt project directory. This is a top-level file and therefore should sit alongside your `dbt_project.yml` file.

## Operation

The Snowplow web model is designed to be run as a whole, which ensures all incremental tables are kept in sync. As such, run the model using:

```bash
dbt run --models snowplow_web tag:snowplow_web_incremental
```

The `snowplow_web` selection will execute all nodes within the Snowplow web package, while the `tag:snowplow_web_incremental` will execute all custom modules that you may have created.

Given the verbose nature of this command we suggest using the YAML selectors we have provided (see section above). The equivalent command using the selector flag would be:

```bash
dbt run --selector snowplow_web
```

### Manifest Tables

There are 3 manifest tables included in this package:

- `snowplow_web_incremental_manifest`: Records the current state of the package.
- `snowplow_web_base_sessions_lifecycle_manifest`: Records the start & end timestamp of all sessions.
- `snowplow_web_base_quarantined_sessions`: Records sessions that have exceeded the maximum allowed session length, defined by `snowplow__max_session_days` (default 3 days).

Please refer to the 'Incremental Logic' section more details on the purpose of each of these tables.

These manifest models are critical to the package **and as such are protected from full refreshes, i.e. being dropped, by default when running in production, while in development refreshes are allowed.**

The `allow_refresh()` macro defines this behavior. As [dbt recommends](https://docs.getdbt.com/faqs/target-names), target names are used here to differentiate between your prod and dev environment. By default, this macro assumes your dev target is named `dev`. This can be changed by setting the `snowplow__dev_target_name` var in your `project.yml` file.

To full refresh any of the manifest models in production, set the `snowplow__allow_refresh` to `true` at run time (see below).

Alternatively, you can amend the behavior of this macro entirely by overwriting it. See the 'Overwriting Macros' section for more details.

### Complete refresh of Snowplow web package

While you can drop and recompute the incremental tables within this package using the standard `--full-refresh` flag, as mentioned above all manifest tables are protected from being dropped in production. Without dropping the manifest during a full refresh, the selected derived incremental tables would be dropped but the processing of events would resume from where the package left off (as captured by the `snowplow_web_incremental_manifest` table) rather than your `snowplow__start_date`.

In order to drop all the manifest tables and start again set the `snowplow__allow_refresh` var to `true` at run time:

```bash
dbt run --models snowplow_web tag:snowplow_web_incremental --full-refresh --vars 'snowplow__allow_refresh: true'
# or using selector flag
dbt run --selector snowplow_web --full-refresh --vars 'snowplow__allow_refresh: true'
```

### Back-filling custom modules

Overtime you may wish to add custom modules to extend the functionality of this package. As you introduce new custom modules into your project, assuming they are tagged correctly (see section on custom modules), the web model will automatically replay all events up until the latest event to have been processed by the other modules.

Note that the batch size of this back-fill is limited as outlined in the 'identification of events to process' section. This means it might take several runs to complete the back-fill, **during which time no new events will be processed by the web model**.

During back-filling, the derived page views, sessions and users tables are blocked from updating. This is to protect against a batched back-fill temporarily introducing incomplete data into these derived tables.

Back-filling a module can be performed either as part of the entire run of the Snowplow web package, or in isolation to reduce cost (recommended):

```bash
dbt run --models snowplow_web tag:snowplow_web_incremental # Will execute all Snowplow web modules, as well as custom.
dbt run --models +my_custom_module # Will execute only your custom module + any upstream nodes.
```

### Tearing down a subset of models

As the code base for your custom modules evolves, you will likely need to replay events through a given module. In order to do so, the models within your custom module need to be removed from the `snowplow_web_incremental_manifest` table. See the 'Complete refresh' section for an explanation as to why. This removal can be achieved by passing the model's name to the `models_to_remove'` var at run time. If you want to replay events through a series of dependent models, you only need to pass the name of the endmost model within the run:

```bash
dbt run --models +snowplow_web_custom_incremental_model --full-refresh --vars 'models_to_remove: snowplow_web_custom_incremental_model'
```

By removing the `snowplow_web_custom_incremental_model` model from the manifest the web packages will be in state 2 and will replay all events.

## Tests

This package contains tests for both the scratch and derived models. Depending on your use case you might not want to run all tests in production, for example to save costs. There are several tags included in the package to help select subsets of tests. Tags:

- `this_run`: Any model with the `_this_run` suffix
- `scratch`: Any model in the scratch sub directories.
- `derived`: Any of the derived models i.e. page views, sessions and users.
- `primary-key`: Any test on the primary keys of all models in this package.

For example if your derived tables are very large you may want to run the full test suite on the `this_run` tables, which act as the input for the derived tables, but only primary key schema tests on the derived tables to ensure no duplicates. If using such a set up, we would also recommend including the `page_view_in_session_value` data test for the page views derived tables.

This is our recommended approach to testing and can be implemented using the selector flag (see YAML selectors section for more details) as follows:

```bash
dbt test --selector snowplow_web_lean_tests
```

This is equivalent to:

```bash
dbt test --models snowplow_web,tag:this_run # Full tests on _this_run models
dbt test --models snowplow_web,tag:manifest # Full tests on manifest models
dbt test --models snowplow_web,tag:primary-key,tag:derived # Primary key tests only on derived tables.
dbt test --models snowplow_web,tag:derived,test_type:data  # Include the page_view_in_session_value data test
```

Alternatively, if you wanted to run all available tests in both the Snowplow web package and your custom modules:

```bash
dbt test --selector snowplow_web
```

## Incremental Logic

The general principle behind an incremental model is to identify new events/rows since the previous run of the model, and then only process these new events. This minimises cost and reduces run times.

For web event data we typically consider a session to be a complete 'visit' and as such calculate metrics across the entire session. This means that when we have a new event for a previously processed session, we have to reprocess all historic events for that session as well as the new events. The logic followed is:

1. Identify new events since the previous run of the package.
2. Identify the `session_id` associated with the new events.
3. Look back over the events table to find all events associated with these `sessions_id`.
4. Run all these events through the page views, sessions and users modules.

Given the large nature of event tables, Step 3. can be an expensive operation. To minimise cost ideally we want to:

- Know when any given session started. This would allow us to limit scans on the events table when looking back for previous events.
  - This is achieved by the `snowplow_web_base_sessions_lifecycle_manifest` model, which records the start and end timestamp of all sessions.
- Limit the maximum allowed session length. Sessions generated by bots can persist for years. This would mean scanning years of data every run of the package.
  - This is achieved by the `snowplow_web_base_quarantined_sessions` model, which stores the `session_id` of any sessions that have exceeded the max allowed session length (`snowplow__max_session_days`).
  - For such sessions, all events are processed up until the max allowed length. Moving forward, no more data is processed for that session.

### snowplow_web_incremental_manifest

This package uses a centralized manifest table, `snowplow_web_incremental_manifest`, to record what events have already been processed and by which model/node. This allows for easy identification of what events to process in subsequent runs of the package. The manifest table is updated as part of an `on-run-end` hook, which calls the `snowplow_incremental_post_hook()` macro.

Example `snowplow_web_incremental_manifest`:

| model                            | last_success |
|----------------------------------|--------------|
| snowplow_web_page_views_this_run | '2021-06-03' |
| snowplow_web_page_views          | '2021-06-03' |
| snowplow_web_sessions            | '2021-06-02' |

### Identification of events to process

The identification of which events to process is performed by the `get_run_limits` macro which is called in the `snowplow_web_base_new_event_limits` model. This macro uses the metadata recorded in `snowplow_web_incremental_manifest` to determine the correct events to process next based on the current state of the Snowplow dbt Web model. The selection of these events is done by specifying a range of `collector_tstamp`'s to process, between `lower_limit` and `upper_limit`. The calculation of these limits is as follows.

First we query `snowplow_web_incremental_manifest`, filtering for all enabled models tagged with `snowplow_web_incremental` within your dbt project:

```sql
select min(last_success) as min_last_success,
       max(last_success) as max_last_success,
       coalesce(count(*), 0) as models
from snowplow_web_incremental_manifest
where model in (array_of_snowplow_tagged_enabled_models)
```

Based on the results the web model enters 1 of 4 states.

#### State 1: First run of the package

The query returns `models = 0` indicating that no models exist in the manifest.

`lower_limit: snowplow__start_date`  
`upper_limit: least(current_tstamp, snowplow__start_date + snowplow__backfill_limit_days)`  

#### State 2: New model introduced

`models < size(array_of_snowplow_tagged_enabled_models)` and therefore a new model, tagged with `snowplow_web_incremental`, has been added since the last run. The package will replay all previously processed events in order to back-fill the new model.

`lower_limit: snowplow__start_date`  
`upper_limit: least(max_last_success, snowplow__start_date + snowplow__backfill_limit_days)`  

#### State 3: Models out of sync

`min_last_success < max_last_success` and therefore the tagged models are out of sync, for example due to a particular model failing to execute successfully during the previous run. The package will attempt to sync all models.

`lower_limit: min_last_success - snowplow__lookback_window_hours`  
`upper_limit: least(max_last_success, min_last_success + snowplow__backfill_limit_days)`  

#### State 4: Standard run

If none of the above criteria are met, then we consider it a 'standard run' and we carry on from the last processed event.

`lower_limit: max_last_success - snowplow__lookback_window_hours`  
`upper_limit: least(current_tstamp, max_last_success + snowplow__backfill_limit_days)`  

**Note in all cases the `upper_limit` is limited by the `snowplow__backfill_limit_days` variable. This protects against back-fills with many rows causing very long run times.**

If you want to check the current state of the web model, run the `snowplow_web_base_new_event_limits` model. This will log the current state to the CLI while causing no disruption to the incremental processing of events.

```bash
dbt run --models snowplow_web_base_new_event_limits
...
00:26:28 | 1 of 1 START table model scratch.snowplow_web_base_new_event_limits.. [RUN]
00:26:29 + Snowplow: Standard incremental run
00:26:29 + Snowplow: Processing data between 2021-01-05 17:59:32 and 2021-01-07 23:59:32
```

## Custom Modules

This package is designed to be easily customised or extended within your own dbt project, by building your own 'custom modules'. The 'standard modules' we provide (base, page views, sessions and users) are not designed to be modified by you. An example dbt project with custom modules can be seen in the [custom example directory](https://github.com/snowplow/dbt-snowplow-web/tree/main/custom_example) of the snowplow-web repo.

### Guidelines & Best Practice

The Snowplow web package's modular structure allows for custom SQL modules to leverage the model's incrementalisation logic, and operate as 'plugins' to compliment the standard model. This can be achieved by using the `_this_run` tables as an input, and producing custom tables which may join to the standard model's main derived tables (for example, to aggregate custom contexts to a page_view level), or provide a separate level of aggregation (for example a custom user interaction).

The standard modules carry out the heavy lifting in establishing an incremental structure and providing the core logic for the most common web aggregation use cases. It also allows custom modules to be plugged in without impeding the maintenance of standard modules.

The following best practices should be followed to ensure that updates and bug fixes to the model can be rolled out with minimal complication:

- Custom modules should not modify any of the tables generated by the Snowplow web package e.g. the scratch, derived or manifest tables.
- Customisations should not modify the SQL provided by the package - they should only comprise of a new set of SQL statements, which produce a separate table.
- The logic for custom SQL should be idempotent, and restart-safe - in other words, it should be written in such a way that a failure mid-way, or a re-run of the model will not change the deterministic output.

In short, the standard modules can be treated as the source code for a distinct piece of software, and custom modules can be treated as self-maintained, additive plugins - in much the same way as a Java package may permit one to leverage public classes in their own API, and provide an entry point for custom programs to run, but will not permit one to modify the original API.

The `_this_run` and derived (e.g. `snowplow_web_page_views`, `snowplow_web_sessions`, `snowplow_web_users`) tables are considered part of the 'public' class of tables in this model structure, and so we can give assurances that non-breaking releases won't alter them. The other tables may be used in custom SQL, but their logic and structure may change from release to release, or they may be removed. If one does use a scratch table in custom logic, any breaking changes can be mitigated by either amending the custom logic to suit, or copying the relevant steps from an old version of the model into the custom module. (However this will rarely be necessary).

### What denotes a custom module?

**Does:**  
In short, anything that plugs into the incremental framework provided by this package. Generally speaking any models you create that reference any of the `_this_run` tables from the standard modules are leveraging this framework and therefore need to be tagged with `snowplow_web_incremental` (see the tagging section). Such models will typically be materialized as incremental, although for more complex custom modules there may be a series of staging models that ultimately produce a derived incremental model. In this case, all staging models also need to be tagged correctly.

**Doesn't:**  
Models that only reference a Snowplow web derived table as their input, rather than a `_this_run` table. Since these derived tables are materialized as incremental they contain all historic events. Any models you build that reference these tables can therefore by written in a drop and recompute manner i.e. materialized as a table. This means they do not leverage the incremental framework of this package and therefore **should not be tagged.**

### Inputs for custom modules

Listed below are the recommended tables to reference as your input for a custom module, depending on the level of aggregation required:

- Event level: `snowplow_web_base_events_this_run`
- Page view level: `snowplow_web_page_views_this_run`
- Session level: `snowplow_web_sessions_this_run`
- User level: `snowplow_web_users_this_run`

### Tagging models

All models within custom modules need to be tagged with `snowplow_web_incremental` in order to leverage the incremental logic of this package. We recommend creating a sub directory of your `/models` directory to contain all your custom modules. In this example we created the sub directory `snowplow_web_custom_modules`. We can then apply the tag to all models in this directory:

```yml
# dbt_project.yml
models:
  your_dbt_project:
    snowplow_web_custom_modules:
      +tags: snowplow_web_incremental #Adds tag to all models in the 'snowplow_web_custom_modules' directory
```

### Retiring Custom Modules

If you want to retire a custom module, you should:

- Delete the models from your project or [disable][dbt-disable-model] the models.
- Not worry about removing the models from the `snowplow_web_incremental_manifest` manifest table. The package identifies **enabled** models tagged with `snowplow_web_incremental` within your project and selects these models from the manifest in order to calculate the state of the web model as described above.
- Not simply exclude the retired models from your Snowplow web job in production. Currently the package is unable to identify which models are due to be executed in a given run. As a result, if you exclude a model the package will get stuck in State 3 as outlined in the identification of events to process section and continue to attempt to sync your excluded with the remaining models.

### Backfilling

We have created a macro `snowplow_utils.is_run_with_new_events(package_name)`, which will evaluate whether the particular model i.e. {{ this }} has already processed the events in the given run of the model. This is returned as a boolean and effectively blocks the upsert to incremental models if the run only contains old data. This protects against your derived incremental tables being temporarily updated with incomplete data during batched back-fills of other models. We recommend including this in the where clause of any incremental models you create within custom modules as follows:

```sql
select
  ...
from 
where {{ snowplow_utils.is_run_with_new_events("snowplow_web") }}
```

A full example of this can be seen in the `custom_example` directory.

### Tips for developing custom modules

While developing custom modules you may benefit from the following:

- Minimising the amount of data being processed to reduce cost & run time.
- Use recent events from your events table to ensure you have all the latest contexts and event types available.
- BigQuery: Automatic handling of evolving schemas for custom contexts and unstructured events.

#### Reducing Costs

By setting `snowplow__backfill_limit_days` to 1 in your `dbt_project.yml` file you will only process a days worth of data per run.

We have provided the `get_value_by_target` macro to dynamically switch the backfill limit depending on your environment i.e. dev vs. prod, with your environment determined by your target name:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__backfill_limit_days: "{{ snowplow_utils.get_value_by_target(
                                            dev_value=1,
                                            default_value=30,
                                            dev_target_name='dev') }}"
```

#### Using Recent Data

This can be achieved by setting `snowplow__start_date` to a recent date. To dynamically change the start date depending on your environment, you can use the following:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__start_date: "{{ snowplow_utils.get_value_by_target(
                                      dev_value=snowplow_utils.n_timedeltas_ago(1, 'weeks'),
                                      default_value='2020-01-01', 
                                      dev_target_name='dev') }}"
```

#### Handling of schema evolution

As your schemas for such custom contexts and unstructured events evolve, multiple versions of the same column will be created in your events table e.g. `custom_context_1_0_0`, `custom_context_1_0_1`. These columns contain nested fields i.e. are of a datatype `RECORD`. When modeling Snowplow data it can be useful to combine or coalesce each nested field across all versions of the column for a continuous view over time.

The snowplow-utils package provides the [combine_column_versions](https://github.com/snowplow/dbt-snowplow-utils#combine_column_versions-source) macro, which will automatically coalesce the fields within every version of the specified column. This mitigates the need for you to update your models every time a new column version is created.

Please refer to the [snowplow-utils][snowplow-utils] docs for the full documentation on these macros.

## User Mapping

This package contains a User Mapping module that aims to link user identifiers, namely `domain_userid` to `user_id`. The logic is to take the latest `user_id` per `domain_userid`.

The `domain_userid` is cookie based and therefore expires over time, where as `user_id` is typically populated when a user logs in with your own internal identifier (dependent on your tracking implementation).

This mapping is applied to the sessions table by a post-hook which updates the `stitched_user_id` column with the latest mapping. If no mapping is present, the default value for `stitched_user_id`  is the `domain_userid`. This process is known as session stitching, and effectively allows you to attribute logged-in and non-logged-in sessions back to a single user.

If required, this update operation can be disabled by setting in your `project.yml` file:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__session_stitching: false
```

User mapping is typically not a 'one size fits all' exercise. Depending on your tracking implementation, business needs and desired level of sophistication you may want to write bespoke logic. Please refer to this [blog post][user-mapping-blog] for ideas.

## Incremental Materialization

This package makes use of the `snowplow_incremental` materialization from the `snowplow_utils` package for the incremental models. This builds upon the out-of-the-box incremental materialization provided by dbt. Its key advantage is that it limits table scans on the target table when updating/inserting based on the new data. This improves performance and reduces cost.

As is the case with the native incremental materialization, the strategy varies between adapters.

Please refer to the [snowplow-utils][snowplow-utils] docs for the full documentation on `snowplow_incremental` materialization.

### Notes

- If using this the `snowplow_incremental` materialization, the native dbt `is_incremental()` macro will not recognise the model as incremental. Please use the `snowplow_utils.snowplow_is_incremental()` macro instead, which operates in the same way.
- If you would rather use an alternative incremental materialization for all incremental models within the package, set the variable `snowplow__incremental_materialization` to your preferred materialization. See the 'Configuration' section for more details.

## Advanced Usage

### Asynchronous Runs

You may wish to run the modules asynchronously, for instance run the page views module hourly but the sessions and users modules daily. You would assume this could be achieved using:

```bash
dbt run --models +snowplow_web.page_views
```

Currently however it is not possible during a dbt jobs start phase to deduce exactly what models are due to be executed from such a command. This means the package is unable to select the subset of models from the manifest. Instead all models from the standard and custom modules are selected from the manifest and the package will attempt to synchronise all models. This makes the above command unsuitable for asynchronous runs.

We can leverage dbt's `ls` in conjunction with shell substitution however to explicitly state what models to run, allowing a subset of models to be selected from the manifest and thus run the page views models independently. To run just the page views module asynchronously:

```bash
dbt run --model +snowplow_web.page_views --vars "{'models_to_run': '$(dbt ls --m  +snowplow_web.page_views --output name)'}"
```

### Cluster Keys

All the incremental models in the Snowplow web package have recommended cluster keys applied to them. Depending on your specific use case, you may want to change or disable these all together. This can be achieved by overriding the following macros with your own version within your project:

- `cluster_by_fields_sessions_lifecycle()`
- `cluster_by_fields_page_views()`
- `cluster_by_fields_sessions()`
- `cluster_by_fields_users()`

### Overriding Macros

Both the cluster key macros (see above) and the `allow_refresh()` macro can be overridden. These are both [dispatched macros](https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch) and can be overridden by creating your own version of the macro and setting a project level dispatch config. More details can be found in [dbt's docs](https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch#overriding-package-macros)

## Duplicates

This package performs de-duplication on both `event_id`'s and `page_view_id`'s, in the base and page views modules respectively. The de-duplication method for Redshift and Postgres is different to BigQuery & Snowflake due to their federated table design. The key difference between the two methodologies is that for Redshift and Postgres an `event_id` may be removed entirely during de-duplication, where as for BigQuery & Snowflake we keep all `event_id`'s. See below for a detailed explanation.

### Redshift & Postgres
Using `event_id` de-duplication as an example, for duplicates we:

- Keep the first row per `event_id` ordered by `collector_tstamp` i.e. the earliest occurring row.
- If there are multiple rows with the same `collector_tstamp`, *we discard the event all together*. This is done to avoid 1:many joins when joining on context tables such as the page view context.

The same methodology is applied to `page_view_id`s, however we order by `derived_tstamp`.

### BigQuery & Snowflake

Using `event_id` de-duplication as an example, for duplicates we:

- Keep the first row per `event_id` ordered by `collector_tstamp` i.e. the earliest occurring row.

The same methodology is applied to `page_view_id`s, however we order by `derived_tstamp`.

# Join the Snowplow community

We welcome all ideas, questions and contributions!

For support requests, please use our community support [Discourse][discourse] forum.

If you find a bug, please report an issue on GitHub.

# Copyright and license

The snowplow-web package is Copyright 2021 Snowplow Analytics Ltd.

Licensed under the [Apache License, Version 2.0][license] (the "License");
you may not use this software except in compliance with the License.

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

[license]: http://www.apache.org/licenses/LICENSE-2.0
[license-image]: http://img.shields.io/badge/license-Apache--2-blue.svg?style=flat
[tracker-classificiation]: https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/tracker-maintenance-classification/
[early-release]: https://img.shields.io/static/v1?style=flat&label=Snowplow&message=Early%20Release&color=014477&labelColor=9ba0aa&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAAAeFBMVEVMaXGXANeYANeXANZbAJmXANeUANSQAM+XANeMAMpaAJhZAJeZANiXANaXANaOAM2WANVnAKWXANZ9ALtmAKVaAJmXANZaAJlXAJZdAJxaAJlZAJdbAJlbAJmQAM+UANKZANhhAJ+EAL+BAL9oAKZnAKVjAKF1ALNBd8J1AAAAKHRSTlMAa1hWXyteBTQJIEwRgUh2JjJon21wcBgNfmc+JlOBQjwezWF2l5dXzkW3/wAAAHpJREFUeNokhQOCA1EAxTL85hi7dXv/E5YPCYBq5DeN4pcqV1XbtW/xTVMIMAZE0cBHEaZhBmIQwCFofeprPUHqjmD/+7peztd62dWQRkvrQayXkn01f/gWp2CrxfjY7rcZ5V7DEMDQgmEozFpZqLUYDsNwOqbnMLwPAJEwCopZxKttAAAAAElFTkSuQmCC

[tracker-docs]: https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/
[docs-what-is-dm]: https://docs.snowplowanalytics.com/docs/modeling-your-data/what-is-data-modeling/
[docs-data-models]: https://docs.snowplowanalytics.com/docs/modeling-your-data/
[dbt-disable-model]: https://docs.getdbt.com/reference/resource-configs/enabled#disable-a-model-in-a-package-in-order-to-use-your-own-version-of-the-model
[dbt-package-docs]: https://docs.getdbt.com/docs/building-a-dbt-project/package-management
[discourse]: http://discourse.snowplowanalytics.com/
[dbt-selectors]: https://docs.getdbt.com/reference/node-selection/yaml-selectors
[selectors-yml-file]: https://github.com/snowplow/dbt-snowplow-web/blob/main/selectors.yml
[dbt-bq-merge-strategy]: https://docs.getdbt.com/reference/resource-configs/bigquery-configs#the-merge-strategy
[dbt-snowflake-merge-strategy]: https://docs.getdbt.com/reference/resource-configs/snowflake-configs#merge-behavior-incremental-models
[snowflake-merge-duplicates]: https://docs.snowflake.com/en/sql-reference/sql/merge.html#duplicate-join-behavior
[snowplow-utils]: https://github.com/snowplow/dbt-snowplow-utils
[user-mapping-blog]: https://snowplowanalytics.com/blog/2021/02/24/developing-a-single-customer-view-with-snowplow/

{% endraw %}
{% enddocs %}
